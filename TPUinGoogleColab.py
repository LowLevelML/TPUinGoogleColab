# -*- coding: utf-8 -*-
"""TensorflowTutorialNotes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sx68upga-g9vCjS1m0N6nbB8woEBcN0r

# Use TPU in Tensorflow colab

GPU OPTIONAL 

So here I downloaded tensorflow gpu to help me use tensorflow on gpu for faster compute times.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install tensorflow
# %pip install tensorflow-gpu

"""# Start of coding bits

Firstly I am importing tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
import os

"""# Initalize

now I am initalizing the TPU and setting the device parameter. Before you start enable TPU in your runtime in google colabs.
"""

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
# This is the TPU initialization code that has to be at the beginning.
tf.tpu.experimental.initialize_tpu_system(resolver)
print("All devices: ", tf.config.list_logical_devices('TPU'))

# set device as tpu 
tf.device('/TPU:0')

"""# Distribution Strategy

"Usually you run your model on multiple TPUs in a data-parallel way. To distribute your model on multiple TPUs (or other accelerators), TensorFlow offers several distribution strategies. You can replace your distribution strategy and the model will run on any given (TPU) device. Check the distribution strategy guide for more information." from [https://www.tensorflow.org/guide/tpu](https://www.tensorflow.org/guide/tpu) explains it well
"""

tfd = tf.distribute.TPUStrategy(resolver)

"""# Testing

let us test out the TPU with the tfd distribution strategy
"""

a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])

@tf.function
def matmul_fn():
  c = tf.matmul(a, b)
  return c

z = tfd.run(matmul_fn)

print(z)